# -*- coding: utf-8 -*-
"""Task_Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STqN3R-veXd46HeOKo4n2XS8MPqfMzMm
"""

!pip install transformers torch scikit-learn --quiet

#Load and Explore the dataset
import json

# Load the JSON file
with open('intents.json') as f:
    data = json.load(f)

# See what the structure looks like
import pprint
pprint.pprint(list(data.items())[:2])

#Convert it into datafram
import pandas as pd

df = pd.DataFrame(data)
df.head()

# Expand the 'intents' column
intents = df['intents'].apply(pd.Series)

# Create a new DataFrame with 'text' and 'label'
rows = []
for _, row in intents.iterrows():
    tag = row['tag']
    patterns = row['patterns']
    for pattern in patterns:
        rows.append({"text": pattern, "label": tag})

train_df = pd.DataFrame(rows)
train_df.head()

# STEP2 Tokenize with HuggingFace Transformers
!pip install transformers datasets

from sklearn.preprocessing import LabelEncoder
from transformers import DistilBertTokenizerFast

# Encode labels to integers
le = LabelEncoder()
train_df['label_encoded'] = le.fit_transform(train_df['label'])

# Tokenizer #uncased ‚Üí the model doesn‚Äôt care about upper/lowercase.
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Tokenize the text
encodings = tokenizer(list(train_df['text']), truncation=True, padding=True)
#truncation=True ‚Üí cuts long sentences to fit the model‚Äôs max length (usually 512 tokens)
#padding=True ‚Üí adds [PAD] tokens so all sequences have the same length
print(encodings.keys)

import torch # required for building deep learning models

class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # Return a single sample (dict)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Create the dataset
dataset = IntentDataset(encodings, train_df['label_encoded'].tolist())

from transformers import DistilBertForSequenceClassification

# Get the number of unique labels
num_labels = len(le.classes_)

# Load the model with the correct number of output classes
model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=num_labels
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=4,
    per_device_train_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

!wandb login YOUR_API_KEY
trainer.train()

#Make Predictions
def predict_intent(message):
    # Tokenize the new message
    encoding = tokenizer(message, truncation=True, padding=True, return_tensors='pt')

    # Get the model's prediction
    with torch.no_grad():
        outputs = model(**encoding)
        logits = outputs.logits

    # Get the predicted class (highest logit)
    predicted_class = torch.argmax(logits, dim=-1).item()

    # Decode the predicted class to its intent label
    predicted_intent = le.inverse_transform([predicted_class])[0]
    return predicted_intent

# Example messages
messages = [
    "Hello, how are you?",
    "I'm feeling down today.",
    "Good morning, everyone!",
    "I'm not sure what to do."
]

# Predict the intent for each message
for msg in messages:
    print(f"Message: {msg} | Predicted Intent: {predict_intent(msg)}")

from sklearn.metrics import accuracy_score, classification_report

def evaluate_model(dataset):
    # Prepare the inputs and labels
    inputs = [tokenizer(text, truncation=True, padding=True, return_tensors='pt') for text in train_df['text']]
    labels = train_df['label_encoded'].tolist()

    # Make predictions for all inputs
    predictions = []
    for encoding in inputs:
        with torch.no_grad():
            outputs = model(**encoding)
            logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=-1).item()
        predictions.append(predicted_class)

    # Calculate accuracy
    accuracy = accuracy_score(labels, predictions)
    print(f"Accuracy: {accuracy}")

    # Detailed classification report (precision, recall, F1-score)
    print("\nClassification Report:")
    print(classification_report(labels, predictions, target_names=le.classes_))

# Evaluate the model
evaluate_model(dataset)

print(num_labels)
print(train_df)

intent_responses = {
    "greeting": "Hey there! How can I help you today? üòä",
    "sadness": "I'm really sorry you're feeling this way. Want to talk about it?",
    "happy": "That's great to hear! Tell me more!",
    "morning": "Good morning! Hope you slept well!",
    "evening": "Good evening! How was your day?",
    "thanks": "You're very welcome! üôè",
    "default": "I'm not sure I understand. Could you rephrase that?"
}

def chatbot_response(user_input):
    intent = predict_intent(user_input)  # Predict the intent from the message
    response = intent_responses.get(intent, intent_responses["default"])
    return response

while True:
    user_input = input("You: ")
    if user_input.lower() in ['quit', 'exit']:
        print("Bot: Goodbye! üëã")
        break
    print("Bot:", chatbot_response(user_input))